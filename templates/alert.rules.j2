groups:
  - name: default
    rules:
    - alert: TCPCheckFailed
      for: 10s
      expr: "probe_success{job='tcpcheck'} == 0"
      labels:
        severity: low
        instance: "{% raw %}{{ $labels.instance }}{% endraw %}"
      annotations:
        message: "{% raw %}{{ $labels.service}} {{ $labels.instance }} on {{ $labels.host }}: TCP check failed{% endraw %}"

    # Score check alert definitions
    - alert: ScoreCheckFailed
      expr: netdata_openio_score__average == 0
      for: 10s
      labels:
        host: "{% raw %}{{ $labels.instance }}{% endraw %}"
        instance: "{% raw %}{{ $labels.s_instance }}{% endraw %}"
        severity: low
      annotations:
        message: "{% raw %}{{ $labels.service }} {{ $labels.s_instance }} on {{ $labels.instance }}: Score is 0{% endraw %}"

    # Per service check alert definitions
    {% for service in prometheus_blackbox_services -%}
    - alert: {{ service }}HealthCheck
      expr: probe_success{job='{{ service }}'} == 0
      for: 10s
      labels:
        service: "{{ service }}"
        severity: low
      annotations:
        message: "{{ service }}{% raw %} {{ $labels.instance }} on {{ $labels.host }}: Health check failed{% endraw %}"
    {% endfor %}

    # RAWX: too many 4xx
    - alert: RawxClientErrorsRatio
      expr: "sum(netdata_openio_rep_hits_4xx__average) by (instance, dimension) / (sum(netdata_openio_rep_hits_2xx__average) by (instance, dimension) + 1) > 0.10"
      for: 10s
      labels:
        host: "{% raw %}{{ $labels.instance }}{% endraw %}"
        instance: "{% raw %}{{ $labels.s_instance }}{% endraw %}"
        service: "rawx"
        severity: low
      annotations:
        message: "{% raw %}{{ $labels.service }} {{ $labels.s_instance }} on {{ $labels.instance }}: Client error (4xx) ratio too high{% endraw %}"

    # RAWX: 5xx errors
    - alert: RawxServerErrors
      expr: "netdata_openio_rep_hits_5xx__average > 0"
      for: 10s
      labels:
        host: "{% raw %}{{ $labels.instance }}{% endraw %}"
        instance: "{% raw %}{{ $labels.s_instance }}{% endraw %}"
        service: "rawx"
        severity: low
      annotations:
        message: "{% raw %}{{ $labels.service }} {{ $labels.s_instance }} on {{ $labels.host }}: Service returned server errors{% endraw %}"

    # Diskspace alerts
    - alert: Diskspace
      expr: "9 * sum(netdata_openio_byte_avail__average) by (s_instance, instance)  < sum(netdata_openio_byte_used__average) by (s_instance, instance)"
      for: 10s
      labels:
        host: "{% raw %}{{ $labels.instance }}{% endraw %}"
        instance: "{% raw %}{{ $labels.s_instance }}{% endraw %}"
        # service: "rawx"
        severity: low
      annotations:
        message: "{% raw %} {{ $labels.s_instance }} on {{ $labels.instance }}: Drive is almost full ( > 90% ) {% endraw %}"

    # Host down alerts
    - alert: HostDown
      expr: "probe_success{job='icmpcheck'} == 0"
      for: 10s
      labels:
        severity: low
      annotations:
        message: "{% raw %} Host {{ $labels.instance }} is unreachable {% endraw %}"

    # Service SLA - Warning
    {% for service in prometheus_blackbox_services -%}
    - alert: {{ service }}SLAWarning
      expr: "100 - (100 * count(count by(instance, service) (ALERTS{severity='low', alertstate='firing', service='{{ service }}'})) / count(probe_success{job='tcpcheck',service='{{ service }}'})) < {{ prometheus_alert_sla_levels_medium[service] if service in prometheus_alert_sla_levels_medium else prometheus_alert_sla_default_medium }}"
      for: 10s
      labels:
        service: "{{ service }}"
        severity: medium
      annotations:
        message: "{{ service }} SLA is below an acceptable level; restore some {{ service }} instances to a functional level"
    {% endfor %}

    # Service SLA - Critical
    {% for service in prometheus_blackbox_services -%}
    - alert: {{ service }}SLACritical
      expr: "100 - (100 * count(count by(instance, service) (ALERTS{severity='low', alertstate='firing', service='{{ service }}'})) / count(probe_success{job='tcpcheck',service='{{ service }}'})) < {{ prometheus_alert_sla_levels_high[service] if service in prometheus_alert_sla_levels_high else prometheus_alert_sla_default_high }}"
      for: 10s
      labels:
        service: "{{ service }}"
        severity: medium
      annotations:
        message: "{{ service }} SLA is below critical level; restore {{ service }} instances to a functional level"
    {% endfor %}
